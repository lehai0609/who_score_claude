# ============================================================================
# FILE: config.py (UPDATED FOR CRAWL4AI 0.6.3)
# ============================================================================

import os
from dotenv import load_dotenv

load_dotenv()

# LLM Configuration
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o")
API_TOKEN = os.getenv("OPENAI_API_KEY", "")

# Target URL
MATCH_URL = os.getenv("MATCH_URL", "https://www.whoscored.com/matches/1821372/live/england-premier-league-2024-2025-nottingham-forest-brentford")

# CSS Selectors
PRIMARY_SELECTOR = "div#matchcentre-timeline-minutes"
ALTERNATIVE_SELECTORS = [
    "div.match-centre-container",
    "div.timeline-container",
    "div#timeline",
    "div#match-centre",
    "[class*='timeline']",
    "[class*='matchcentre']"
]

CONTEXT_SELECTORS = [
    "div#match-centre",
    "div#match-centre-header", 
    "div#match-centre-content",
    "div.match-header",
    "div.score-box"
]

# Scraper Instructions for LLM
SCRAPER_INSTRUCTIONS = """
You are analyzing a WhoScored.com match centre page to extract timeline rating data.

EXTRACT the following information:
1. **Match Information**: Team names, score, date, competition
2. **Timeline Data**: Player/team ratings by minute/period
3. **Match Events**: Goals, cards, substitutions with exact timestamps  
4. **Rating Changes**: How ratings evolve throughout the match
5. **Performance Metrics**: Key statistics by time period

IGNORE completely:
- Advertisement content
- Promotional banners
- Social media widgets
- Navigation menus
- Cookie notices
- Pop-up overlays

FOCUS on:
- Data from the match centre timeline container
- Numerical ratings and statistics
- Timestamped events and changes
- Performance data tables

Return clean, structured JSON with timeline progression and rating data.
"""

# Configuration
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
REQUEST_DELAY = float(os.getenv("REQUEST_DELAY", "2"))
TIMEOUT_DURATION = int(os.getenv("TIMEOUT_DURATION", "60"))
OUTPUT_FILE = "match_timeline_data.json"

# ============================================================================
# FILE: src/scraper.py (UPDATED FOR CRAWL4AI 0.6.3)
# ============================================================================

import asyncio
import json
from typing import Dict, Any, Optional, Type
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from config import (
    LLM_MODEL, API_TOKEN, PRIMARY_SELECTOR, ALTERNATIVE_SELECTORS, 
    CONTEXT_SELECTORS, SCRAPER_INSTRUCTIONS, MAX_RETRIES, REQUEST_DELAY
)
from src.utils import clean_extracted_data, handle_extraction_error

def get_browser_config() -> BrowserConfig:
    """
    Configure browser for WhoScored.com scraping.
    Updated for Crawl4AI 0.6.3 API.
    """
    return BrowserConfig(
        headless=True,
        viewport_width=1280,
        viewport_height=800,
        verbose=True,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

def get_llm_strategy() -> LLMExtractionStrategy:
    """
    Configure LLM extraction strategy for Crawl4AI 0.6.3.
    """
    return LLMExtractionStrategy(
        provider=LLM_MODEL,
        api_token=API_TOKEN,
        instructions=SCRAPER_INSTRUCTIONS,
        output_format="json",
        extra_args={
            "temperature": 0.1,
            "max_tokens": 4000
        }
    )

async def check_page_loaded(crawler: AsyncWebCrawler, url: str) -> bool:
    """
    Verify that the WhoScored page has loaded properly.
    """
    try:
        # Simple check to see if page loads
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            wait_for=f"() => document.querySelector('{PRIMARY_SELECTOR}') !== null || document.querySelector('div.match-centre-container') !== null",
            delay_before_return_html=3.0
        )
        
        result = await crawler.arun(url=url, config=run_config)
        
        if result.success:
            # Check for key indicators
            content = result.cleaned_html.lower()
            indicators = ["match", "timeline", "rating", "vs", "score"]
            found_indicators = sum(1 for indicator in indicators if indicator in content)
            return found_indicators >= 2
            
        return False
        
    except Exception as e:
        print(f"Error checking page load: {e}")
        return False

async def extract_with_fallback_selectors(
    crawler: AsyncWebCrawler,
    url: str, 
    llm_strategy: LLMExtractionStrategy
) -> Optional[Dict[str, Any]]:
    """
    Attempt data extraction with multiple CSS selectors as fallback.
    Updated for Crawl4AI 0.6.3 API.
    """
    selectors_to_try = [PRIMARY_SELECTOR] + ALTERNATIVE_SELECTORS
    
    for i, selector in enumerate(selectors_to_try):
        try:
            print(f"Attempting extraction with selector {i+1}/{len(selectors_to_try)}: {selector}")
            
            # Build run config for this selector
            run_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=llm_strategy,
                css_selector=selector,
                wait_for=f"() => document.querySelector('{selector}') !== null" if i == 0 else None,
                delay_before_return_html=3.0 if i == 0 else 1.0,
                word_count_threshold=10
            )
            
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success and result.extracted_content:
                try:
                    # Parse the extracted content
                    if isinstance(result.extracted_content, str):
                        extracted_data = json.loads(result.extracted_content)
                    else:
                        extracted_data = result.extracted_content
                        
                    if extracted_data and validate_extracted_data(extracted_data):
                        print(f"‚úÖ Successfully extracted data with selector: {selector}")
                        return extracted_data
                        
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"‚ùå Invalid JSON from selector {selector}: {e}")
                    continue
            else:
                print(f"‚ùå No data extracted with selector: {selector}")
                if result.error_message:
                    print(f"Error: {result.error_message}")
                    
        except Exception as e:
            print(f"‚ùå Exception with selector {selector}: {e}")
            continue
            
        # Brief delay between attempts
        await asyncio.sleep(1)
    
    print("‚ö†Ô∏è All selectors failed - attempting context-based extraction")
    return await extract_with_context_selectors(crawler, url, llm_strategy)

async def extract_with_context_selectors(
    crawler: AsyncWebCrawler,
    url: str,
    llm_strategy: LLMExtractionStrategy
) -> Optional[Dict[str, Any]]:
    """
    Extract data using broader context selectors when specific selectors fail.
    """
    try:
        # Try extracting from broader page context
        combined_selectors = ", ".join(CONTEXT_SELECTORS)
        
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            extraction_strategy=llm_strategy,
            css_selector=combined_selectors,
            word_count_threshold=50
        )
        
        result = await crawler.arun(url=url, config=run_config)
        
        if result.success and result.extracted_content:
            try:
                if isinstance(result.extracted_content, str):
                    extracted_data = json.loads(result.extracted_content)
                else:
                    extracted_data = result.extracted_content
                    
                if extracted_data:
                    print("‚úÖ Extracted data using context selectors")
                    return extracted_data
            except (json.JSONDecodeError, TypeError):
                pass
                
    except Exception as e:
        print(f"Context extraction failed: {e}")
    
    return None

def validate_extracted_data(data: Dict[str, Any]) -> bool:
    """
    Validate that extracted data contains essential match information.
    """
    if not isinstance(data, dict):
        return False
        
    # Check for required fields
    data_str = str(data).lower()
    required_indicators = [
        any(key in data_str for key in ["match", "team", "score", "rating"]),
        len(data_str) > 100,  # Substantial content
        not all(v is None for v in data.values() if v is not None)  # Not all empty
    ]
    
    return all(required_indicators)

async def scrape_whoscored_match(match_url: str, output_format: Type) -> Optional[Dict[str, Any]]:
    """
    Main function to scrape WhoScored match centre data.
    Updated for Crawl4AI 0.6.3 API.
    
    Args:
        match_url: URL of the WhoScored match page
        output_format: Pydantic model class for data structure
        
    Returns:
        Extracted match data or None if extraction fails
    """
    browser_config = get_browser_config()
    llm_strategy = get_llm_strategy()
    
    print(f"üèÜ Starting WhoScored match scraping: {match_url}")
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # First, verify the page loads correctly
        print("üîÑ Checking page load status...")
        page_loaded = await check_page_loaded(crawler, match_url)
        
        if not page_loaded:
            print("‚ö†Ô∏è Page may not have loaded completely, proceeding anyway...")
        
        # Attempt extraction with retry logic
        for attempt in range(MAX_RETRIES):
            try:
                print(f"üéØ Extraction attempt {attempt + 1}/{MAX_RETRIES}")
                
                extracted_data = await extract_with_fallback_selectors(
                    crawler, match_url, llm_strategy
                )
                
                if extracted_data:
                    # Clean and validate the data
                    cleaned_data = clean_extracted_data(extracted_data)
                    print("‚úÖ Match data extracted successfully!")
                    return cleaned_data
                else:
                    print(f"‚ùå Extraction attempt {attempt + 1} failed")
                    
            except Exception as e:
                error_msg = handle_extraction_error(e, attempt + 1)
                print(f"‚ùå {error_msg}")
                
            # Wait before retry
            if attempt < MAX_RETRIES - 1:
                print(f"‚è≥ Waiting {REQUEST_DELAY} seconds before retry...")
                await asyncio.sleep(REQUEST_DELAY)
    
    print("üí• All extraction attempts failed")
    return None

# ============================================================================
# FILE: src/utils.py (UPDATED)
# ============================================================================

import json
import logging
import re
from typing import Any, Dict
from datetime import datetime
from pathlib import Path

def setup_logging(log_level: str = "INFO"):
    """Configure logging for the scraper."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("scraper.log"),
            logging.StreamHandler()
        ]
    )

def validate_match_url(url: str) -> bool:
    """
    Validate that the URL is a proper WhoScored match URL.
    """
    pattern = r"^https://www\.whoscored\.com/matches/\d+/live/.+"
    return re.match(pattern, url) is not None

def clean_extracted_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Clean and normalize extracted data from WhoScored.
    """
    if not isinstance(data, dict):
        return {}
    
    cleaned = {}
    
    for key, value in data.items():
        # Skip error flags
        if key == "error" and value is False:
            continue
            
        # Clean string values
        if isinstance(value, str):
            value = value.strip()
            if value.lower() in ['n/a', 'null', 'none', '']:
                value = None
        
        # Clean numerical strings
        elif isinstance(value, str) and value.replace('.', '').replace('-', '').isdigit():
            try:
                value = float(value) if '.' in value else int(value)
            except ValueError:
                pass
        
        # Recursively clean nested dictionaries
        elif isinstance(value, dict):
            value = clean_extracted_data(value)
            
        # Clean lists
        elif isinstance(value, list):
            value = [clean_extracted_data(item) if isinstance(item, dict) else item 
                    for item in value if item is not None]
        
        if value is not None:
            cleaned[key] = value
    
    return cleaned

def handle_extraction_error(error: Exception, attempt: int) -> str:
    """
    Handle and format extraction errors.
    """
    error_type = type(error).__name__
    error_message = str(error)
    return f"Attempt {attempt} failed - {error_type}: {error_message}"

def format_timeline_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Format raw extracted data into structured timeline format.
    """
    formatted = {
        "match_info": {},
        "timeline_data": [],
        "summary_stats": {}
    }
    
    # Extract basic match info
    if "match_info" in raw_data:
        formatted["match_info"] = raw_data["match_info"]
    elif any(key in raw_data for key in ["home_team", "away_team", "score"]):
        # Try to extract match info from top level
        formatted["match_info"] = {
            "home_team": raw_data.get("home_team"),
            "away_team": raw_data.get("away_team"), 
            "score": raw_data.get("score"),
            "date": raw_data.get("date"),
            "competition": raw_data.get("competition")
        }
    
    # Format timeline data
    if "timeline_ratings" in raw_data:
        formatted["timeline_data"] = raw_data["timeline_ratings"]
    elif "timeline" in raw_data:
        formatted["timeline_data"] = raw_data["timeline"]
    elif "events" in raw_data:
        formatted["timeline_data"] = raw_data["events"]
    
    # Add summary statistics
    if "team_stats" in raw_data:
        formatted["summary_stats"] = raw_data["team_stats"]
    
    return formatted

def save_data_to_json(data: Dict[str, Any], filename: str = "match_data.json") -> bool:
    """
    Save extracted data to JSON file.
    """
    try:
        # Add metadata
        data["extraction_metadata"] = {
            "timestamp": datetime.now().isoformat(),
            "scraper_version": "1.0.1",
            "source": "WhoScored.com",
            "crawl4ai_version": "0.6.3"
        }
        
        # Ensure output directory exists
        output_path = Path(filename)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Save with pretty formatting
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Data saved successfully to: {filename}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error saving data to {filename}: {e}")
        return False

def create_summary_report(data: Dict[str, Any]) -> str:
    """
    Create a text summary report of the extracted data.
    """
    report_lines = [
        "=" * 50,
        "WHOSCORED MATCH CENTRE DATA SUMMARY",
        "=" * 50
    ]
    
    # Match info
    if "match_info" in data and data["match_info"]:
        match_info = data["match_info"]
        report_lines.extend([
            f"Match: {match_info.get('home_team', 'N/A')} vs {match_info.get('away_team', 'N/A')}",
            f"Score: {match_info.get('score', 'N/A')}",
            f"Date: {match_info.get('date', 'N/A')}",
            f"Competition: {match_info.get('competition', 'N/A')}",
            ""
        ])
    
    # Timeline summary
    if "timeline_data" in data and data["timeline_data"]:
        timeline_length = len(data["timeline_data"]) if isinstance(data["timeline_data"], list) else 1
        report_lines.extend([
            "TIMELINE SUMMARY:",
            f"- Timeline entries: {timeline_length}",
            f"- Data points collected throughout match",
            ""
        ])
    
    # Raw data summary
    report_lines.extend([
        "EXTRACTION SUMMARY:",
        f"- Total data fields: {len(data)}",
        f"- Data structure: {list(data.keys())}",
        ""
    ])
    
    # Extraction info
    if "extraction_metadata" in data:
        metadata = data["extraction_metadata"]
        report_lines.extend([
            "EXTRACTION INFO:",
            f"- Timestamp: {metadata.get('timestamp', 'N/A')}",
            f"- Source: {metadata.get('source', 'N/A')}",
            f"- Scraper Version: {metadata.get('scraper_version', 'N/A')}",
            f"- Crawl4AI Version: {metadata.get('crawl4ai_version', 'N/A')}"
        ])
    
    report_lines.append("=" * 50)
    
    return "\\n".join(report_lines)

# ============================================================================
# FILE: main.py (UPDATED)
# ============================================================================

import asyncio
import sys
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from config import MATCH_URL, OUTPUT_FILE
from src.scraper import scrape_whoscored_match
from src.utils import (
    save_data_to_json, 
    validate_match_url, 
    setup_logging,
    format_timeline_data,
    create_summary_report
)
from models.match_data import MatchData

def print_startup_banner():
    print("""
========================================
 WhoScored Match Centre Timeline Scraper
 Updated for Crawl4AI 0.6.3
========================================
    """)

async def main():
    """
    Main entry point for WhoScored match centre scraper.
    """
    print_startup_banner()
    
    # Setup logging
    setup_logging()
    
    # Validate match URL
    if not validate_match_url(MATCH_URL):
        print("‚ùå Invalid WhoScored match URL!")
        print(f"URL: {MATCH_URL}")
        print("Expected format: https://www.whoscored.com/matches/{id}/live/{match-slug}")
        sys.exit(1)
    
    print(f"üéØ Target URL: {MATCH_URL}")
    print(f"üìä Output file: {OUTPUT_FILE}")
    print()
    
    try:
        # Scrape the match data
        print("üöÄ Starting data extraction...")
        extracted_data = await scrape_whoscored_match(MATCH_URL, MatchData)
        
        if extracted_data:
            # Format the data
            formatted_data = format_timeline_data(extracted_data)
            
            # Save to JSON file
            if save_data_to_json(formatted_data, OUTPUT_FILE):
                print(f"‚úÖ Success! Data saved to {OUTPUT_FILE}")
                
                # Create and display summary
                summary = create_summary_report(formatted_data)
                print()
                print(summary)
                
                # Save summary report
                summary_file = OUTPUT_FILE.replace('.json', '_summary.txt')
                with open(summary_file, 'w', encoding='utf-8') as f:
                    f.write(summary)
                print(f"üìã Summary report saved to {summary_file}")
                
            else:
                print("‚ùå Failed to save extracted data")
                sys.exit(1)
        else:
            print("üí• No data was extracted from the match page")
            print()
            print("Possible reasons:")
            print("- Match page not accessible")
            print("- Page structure changed")
            print("- Network connectivity issues")
            print("- Rate limiting or blocking")
            print("- Invalid API configuration")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è Scraping interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"üí• Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def run_scraper():
    """
    Convenience function to run the scraper.
    """
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è Scraping stopped")
    except Exception as e:
        print(f"üí• Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_scraper()